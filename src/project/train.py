import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

import time
from pathlib import Path
import multiprocessing

from project.model import CNN  # –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç, –µ—Å–ª–∏ src –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ Sources Root

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
class Config:
    batch_size = 64
    epochs = 15
    learning_rate = 0.001
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")


# –≤—ã—á–∏—Å–ª—è–µ–º –∫–æ—Ä–µ–Ω—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (–¥–≤–µ –ø–∞–ø–∫–∏ –≤–≤–µ—Ä—Ö –æ—Ç —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞: src/project -> –∫–æ—Ä–µ–Ω—å)
BASE_DIR = Path(__file__).resolve().parents[2]

DATA_DIR = BASE_DIR / "data"
MODEL_DIR = BASE_DIR / "models"
GRAPHICS_DIR = BASE_DIR / "graphics"

# –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫–∏, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
DATA_DIR.mkdir(parents=True, exist_ok=True)
MODEL_DIR.mkdir(parents=True, exist_ok=True)
GRAPHICS_DIR.mkdir(parents=True, exist_ok=True)

# –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤—ã–±–æ—Ä num_workers
_cpu_count = multiprocessing.cpu_count() if hasattr(multiprocessing, "cpu_count") else 1
NUM_WORKERS = min(2, max(0, _cpu_count - 1))  # 0..2

# –§—É–Ω–∫—Ü–∏–∏ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä–æ–≤
def get_data_loaders():
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    train_dataset = datasets.MNIST(
        root=str(DATA_DIR),
        train=True,
        download=True,
        transform=transform
    )

    test_dataset = datasets.MNIST(
        root=str(DATA_DIR),
        train=False,
        download=True,
        transform=transform
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=Config.batch_size,
        shuffle=True,
        num_workers=NUM_WORKERS,
        pin_memory=torch.cuda.is_available()
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=Config.batch_size,
        shuffle=False,
        num_workers=NUM_WORKERS,
        pin_memory=torch.cuda.is_available()
    )

    return train_loader, test_loader

# –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è –∏ —Ç–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏–∏
def train_epoch(model, device, train_loader, optimizer, epoch):
    model.train()
    train_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()

        output = model(data)
        loss = F.nll_loss(output, target)

        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()
        total += target.size(0)

        if batch_idx % 100 == 0:
            processed = batch_idx * len(data)
            percent = 100. * batch_idx / len(train_loader)
            print(f'Train Epoch: {epoch} [{processed}/{len(train_loader.dataset)} '
                  f'({percent:.0f}%)]\tLoss: {loss.item():.6f}')

    avg_loss = train_loss / len(train_loader) if len(train_loader) > 0 else 0.0
    accuracy = 100. * correct / total if total > 0 else 0.0

    return avg_loss, accuracy


def test(model, device, test_loader):
    model.eval()
    test_loss = 0.0
    correct = 0

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            # —Å—É–º–º–∏—Ä—É–µ–º loss –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ
            test_loss += F.nll_loss(output, target, reduction='sum').item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    n_samples = len(test_loader.dataset)
    test_loss_avg = test_loss / n_samples if n_samples > 0 else 0.0
    accuracy = 100. * correct / n_samples if n_samples > 0 else 0.0

    print(f'\nTest set: Average loss: {test_loss_avg:.4f}, '
          f'Accuracy: {correct}/{n_samples} ({accuracy:.2f}%)\n')

    return test_loss_avg, accuracy

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ graphics/)
def visualize_samples(test_loader, model, device, num_samples=10):
    model.eval()
    data_iter = iter(test_loader)
    images, labels = next(data_iter)
    images, labels = images.to(device), labels.to(device)

    with torch.no_grad():
        outputs = model(images[:num_samples])
        preds = outputs.argmax(dim=1)

    images_np = images.cpu().numpy()
    labels_np = labels.cpu().numpy()
    preds_np = preds.cpu().numpy()

    fig, axes = plt.subplots(2, 5, figsize=(12, 6))
    for i, ax in enumerate(axes.flat):
        ax.imshow(images_np[i][0], cmap='gray')
        ax.set_title(f'True: {labels_np[i]}, Pred: {preds_np[i]}')
        ax.axis('off')

    plt.tight_layout()
    out_path = GRAPHICS_DIR / "mnist_predictions.png"
    plt.savefig(str(out_path), dpi=150, bbox_inches='tight')
    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {out_path}")
    plt.close()

# –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
def main():
    cfg = Config()

    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö MNIST...")
    train_loader, test_loader = get_data_loaders()

    print(f"–†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(train_loader.dataset)}")
    print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(test_loader.dataset)}")
    print(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {cfg.batch_size}")

    device = cfg.device

    # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å
    model = CNN().to(device)
    print(f"\n–ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞: {model.__class__.__name__}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters()):,}")

    optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)

    train_losses = []
    train_accuracies = []
    test_losses = []
    test_accuracies = []

    print("\n–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    start_time = time.time()

    for epoch in range(1, cfg.epochs + 1):
        epoch_start = time.time()

        # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞
        train_loss, train_acc = train_epoch(model, device, train_loader, optimizer, epoch)

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        test_loss, test_acc = test(model, device, test_loader)

        epoch_time = time.time() - epoch_start

        train_losses.append(train_loss)
        train_accuracies.append(train_acc)
        test_losses.append(test_loss)
        test_accuracies.append(test_acc)

        print(f"–≠–ø–æ—Ö–∞ {epoch} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {epoch_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {train_acc:.2f}%")
        print(f"–¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {test_acc:.2f}%\n")

    total_time = time.time() - start_time
    print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {total_time:.2f} —Å–µ–∫—É–Ω–¥")

    # –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –≥—Ä–∞—Ñ–∏–∫–∏
    print("–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤...")
    visualize_samples(test_loader, model, device)

    # –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss')
    plt.plot(test_losses, label='Test Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Loss during Training')

    plt.subplot(1, 2, 2)
    plt.plot(train_accuracies, label='Train Accuracy')
    plt.plot(test_accuracies, label='Test Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.title('Accuracy during Training')

    plt.tight_layout()
    history_path = GRAPHICS_DIR / "training_history.png"
    plt.savefig(str(history_path), dpi=150, bbox_inches='tight')
    print(f"–°–æ—Ö—Ä–∞–Ω—ë–Ω –≥—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è: {history_path}")
    plt.close()

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç—Ä–∏–∫ –≤ models/
    save_path = MODEL_DIR / "mnist_cnn_model.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_losses': train_losses,
        'test_losses': test_losses,
        'train_accuracies': train_accuracies,
        'test_accuracies': test_accuracies,
        'config': {
            'batch_size': cfg.batch_size,
            'epochs': cfg.epochs,
            'learning_rate': cfg.learning_rate
        }
    }, str(save_path))

    print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ '{save_path}'")

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if test_accuracies:
        best_test_acc = max(test_accuracies)
        best_epoch = test_accuracies.index(best_test_acc) + 1
        print(f"\nüèÜ –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {best_test_acc:.2f}% –Ω–∞ —ç–ø–æ—Ö–µ {best_epoch}")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
def load_and_test_model():
    cfg = Config()

    _, test_loader = get_data_loaders()
    device = cfg.device

    model = CNN().to(device)
    save_path = MODEL_DIR / "mnist_cnn_model.pth"

    if not save_path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {save_path}")

    checkpoint = torch.load(str(save_path), map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])

    print("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ...")
    test_loss, test_accuracy = test(model, device, test_loader)

    return test_accuracy


if __name__ == '__main__':
    main()

    # –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ—Å–ª–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–∂–Ω–æ —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å:
    load_and_test_model()
